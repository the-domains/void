---
author: []
related: []
publisher:
  url: 'http://torch.ch'
  name: Torch
  favicon: 'http://torch.ch/static/favicon.png'
  domain: torch.ch
keywords:
  - glimpse
  - reinforcement
  - algorithms
  - module
  - input
  - models
  - reward
  - mnist
  - attention
  - recurrent
description: 'In this blog post, I want to discuss how we at Element-Research implemented the recurrent attention model (RAM) described in[1]. Not only were we able to reproduce the paper, but we also made of bunch of modular code available in the process. You can reproduce the RAM on the MNIST dataset using this training script.'
inLanguage: en
app_links: []
isBasedOnUrl: 'http://torch.ch/blog/2015/09/21/rmva.html'
title: 'Torch | Recurrent Model of Visual Attention'
datePublished: '2015-09-23T16:33:06.040Z'
dateModified: '2015-09-23T13:50:50.097Z'
sourcePath: _posts/2015-09-23-torch-or-recurrent-model-of-visual-attention.md
published: true
_context: 'http://schema.org'
_type: MediaObject

---
<article style=""><h1>Torch &amp;vert; Recurrent Model of Visual Attention</h1><p>In this blog post&amp;comma; I want to discuss how we at Element-Research implemented the recurrent attention model &amp;lpar;RAM&amp;rpar; described in&amp;lsqb;1&amp;rsqb;&amp;period; Not only were we able to reproduce the paper&amp;comma; but we also made of bunch of modular code available in the process&amp;period; You can reproduce the RAM on the MNIST dataset using this training script&amp;period;</p><img src="https://raw.githubusercontent.com/torch/torch.github.io/master/blog/_posts/images/rva-diagram.png" /></article>